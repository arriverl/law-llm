#!/usr/bin/env python3
"""
åŸºäºDISC-LawLLMæ¶æ„çš„ç²¤æ¸¯æ¾³å¤§æ¹¾åŒºæ³•å¾‹æ•°æ®æ”¶é›†è„šæœ¬
å‚è€ƒ: https://github.com/FudanDISC/DISC-LawLLM
"""
import os
import sys
import asyncio
import requests
from bs4 import BeautifulSoup
import json
import time
import logging
from datetime import datetime
from typing import List, Dict, Any, Tuple
import pandas as pd
import jieba
from collections import Counter

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DISCLawDataCollector:
    """åŸºäºDISC-LawLLMæ¶æ„çš„æ³•å¾‹æ•°æ®æ”¶é›†å™¨"""
    
    def __init__(self, output_dir="data/disc_law_data"):
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        self.collected_data = []
        
        # ç²¤æ¸¯æ¾³å¤§æ¹¾åŒºæ³•å¾‹åˆ†ç±»ä½“ç³»ï¼ˆå‚è€ƒDISC-LawLLMï¼‰
        self.legal_categories = {
            "æ°‘äº‹": {
                "keywords": ["åˆåŒ", "åè®®", "å±¥è¡Œ", "è¿çº¦", "èµ”å¿", "å€ºåŠ¡", "è´¢äº§", "å©šå§»", "ç»§æ‰¿", "ç¦»å©š", "æŠšå…»"],
                "subcategories": ["åˆåŒæ³•", "å©šå§»æ³•", "ç»§æ‰¿æ³•", "ç‰©æƒæ³•", "ä¾µæƒæ³•"]
            },
            "åˆ‘äº‹": {
                "keywords": ["çŠ¯ç½ª", "åˆ‘ç½š", "åˆ‘äº‹", "èµ·è¯‰", "åˆ¤å†³", "é‡åˆ‘", "ç›—çªƒ", "æŠ¢åŠ«", "æ•…æ„ä¼¤å®³", "è¯ˆéª—"],
                "subcategories": ["åˆ‘æ³•æ€»åˆ™", "åˆ‘æ³•åˆ†åˆ™", "åˆ‘äº‹è¯‰è®¼æ³•", "åˆ‘äº‹æ‰§è¡Œ"]
            },
            "è¡Œæ”¿": {
                "keywords": ["è¡Œæ”¿", "æ”¿åºœ", "å¤„ç½š", "å¤è®®", "è®¸å¯", "å®¡æ‰¹", "æ‰§æ³•", "ç›‘ç®¡", "è¡Œæ”¿å¤„ç½š"],
                "subcategories": ["è¡Œæ”¿æ³•", "è¡Œæ”¿è¯‰è®¼æ³•", "è¡Œæ”¿å¤è®®æ³•", "è¡Œæ”¿è®¸å¯æ³•"]
            },
            "å•†äº‹": {
                "keywords": ["å…¬å¸", "è‚¡ä¸œ", "è‘£äº‹", "ä¼ä¸š", "æŠ•èµ„", "è´¸æ˜“", "è‚¡æƒ", "å¹¶è´­", "ç ´äº§"],
                "subcategories": ["å…¬å¸æ³•", "è¯åˆ¸æ³•", "ä¿é™©æ³•", "ç ´äº§æ³•", "æµ·å•†æ³•"]
            },
            "åŠ³åŠ¨": {
                "keywords": ["åŠ³åŠ¨", "å‘˜å·¥", "å·¥èµ„", "ç¤¾ä¿", "å·¥ä¼¤", "è§£é›‡", "åŠ ç­", "ä¼‘å‡", "åŠ³åŠ¨åˆåŒ"],
                "subcategories": ["åŠ³åŠ¨æ³•", "åŠ³åŠ¨åˆåŒæ³•", "ç¤¾ä¼šä¿é™©æ³•", "åŠ³åŠ¨äº‰è®®è°ƒè§£ä»²è£æ³•"]
            },
            "çŸ¥è¯†äº§æƒ": {
                "keywords": ["ä¸“åˆ©", "å•†æ ‡", "ç‰ˆæƒ", "çŸ¥è¯†äº§æƒ", "ä¾µæƒ", "å‘æ˜", "åˆ›ä½œ", "è‘—ä½œæƒ"],
                "subcategories": ["ä¸“åˆ©æ³•", "å•†æ ‡æ³•", "è‘—ä½œæƒæ³•", "åä¸æ­£å½“ç«äº‰æ³•"]
            },
            "ç¯å¢ƒ": {
                "keywords": ["ç¯å¢ƒ", "æ±¡æŸ“", "ä¿æŠ¤", "ç”Ÿæ€", "æ’æ”¾", "æ²»ç†", "å¯æŒç»­å‘å±•", "ç¯ä¿"],
                "subcategories": ["ç¯å¢ƒä¿æŠ¤æ³•", "å¤§æ°”æ±¡æŸ“é˜²æ²»æ³•", "æ°´æ±¡æŸ“é˜²æ²»æ³•", "å›ºä½“åºŸç‰©æ±¡æŸ“ç¯å¢ƒé˜²æ²»æ³•"]
            },
            "å›½é™…": {
                "keywords": ["å›½é™…", "ä»²è£", "è´¸æ˜“", "æŠ•èµ„", "åˆä½œ", "æ¡çº¦", "å¤–äº¤", "å›½é™…æ³•"],
                "subcategories": ["å›½é™…å…¬æ³•", "å›½é™…ç§æ³•", "å›½é™…ç»æµæ³•", "å›½é™…å•†æ³•"]
            },
            "é‡‘è": {
                "keywords": ["é“¶è¡Œ", "è¯åˆ¸", "ä¿é™©", "é‡‘è", "æŠ•èµ„", "ç†è´¢", "è´·æ¬¾", "ä¿¡è´·"],
                "subcategories": ["é“¶è¡Œæ³•", "è¯åˆ¸æ³•", "ä¿é™©æ³•", "ä¿¡æ‰˜æ³•", "åŸºé‡‘æ³•"]
            },
            "å…¶ä»–": {
                "keywords": ["å…¶ä»–", "ç»¼åˆ", "ä¸€èˆ¬", "ç‰¹æ®Š"],
                "subcategories": ["å…¶ä»–æ³•å¾‹", "ç»¼åˆæ³•å¾‹", "ç‰¹æ®Šæ³•å¾‹"]
            }
        }
    
    def collect_gba_court_data(self):
        """æ”¶é›†ç²¤æ¸¯æ¾³å¤§æ¹¾åŒºæ³•é™¢æ•°æ®"""
        logger.info("ğŸ›ï¸ å¼€å§‹æ”¶é›†ç²¤æ¸¯æ¾³å¤§æ¹¾åŒºæ³•é™¢æ•°æ®...")
        
        # ç²¤æ¸¯æ¾³å¤§æ¹¾åŒºæ³•é™¢ç½‘ç«™åˆ—è¡¨
        gba_courts = [
            {
                "name": "å¹¿ä¸œçœé«˜çº§äººæ°‘æ³•é™¢",
                "url": "http://www.gdcourts.gov.cn/",
                "region": "å¹¿ä¸œ",
                "selectors": {
                    "title": "h1, h2, h3, .title, .news-title, .article-title",
                    "content": "p, .content, .article-content, .news-content, .text"
                }
            },
            {
                "name": "æ·±åœ³å¸‚ä¸­çº§äººæ°‘æ³•é™¢",
                "url": "http://www.szcourt.gov.cn/",
                "region": "æ·±åœ³",
                "selectors": {
                    "title": "h1, h2, h3, .title, .news-title, .article-title",
                    "content": "p, .content, .article-content, .news-content, .text"
                }
            },
            {
                "name": "å¹¿å·å¸‚ä¸­çº§äººæ°‘æ³•é™¢",
                "url": "http://www.gzcourt.gov.cn/",
                "region": "å¹¿å·",
                "selectors": {
                    "title": "h1, h2, h3, .title, .news-title, .article-title",
                    "content": "p, .content, .article-content, .news-content, .text"
                }
            },
            {
                "name": "ç æµ·å¸‚ä¸­çº§äººæ°‘æ³•é™¢",
                "url": "http://www.zhcourt.gov.cn/",
                "region": "ç æµ·",
                "selectors": {
                    "title": "h1, h2, h3, .title, .news-title, .article-title",
                    "content": "p, .content, .article-content, .news-content, .text"
                }
            },
            {
                "name": "ä½›å±±å¸‚ä¸­çº§äººæ°‘æ³•é™¢",
                "url": "http://www.fscourt.gov.cn/",
                "region": "ä½›å±±",
                "selectors": {
                    "title": "h1, h2, h3, .title, .news-title, .article-title",
                    "content": "p, .content, .article-content, .news-content, .text"
                }
            },
            {
                "name": "ä¸œèå¸‚ä¸­çº§äººæ°‘æ³•é™¢",
                "url": "http://www.dgcourt.gov.cn/",
                "region": "ä¸œè",
                "selectors": {
                    "title": "h1, h2, h3, .title, .news-title, .article-title",
                    "content": "p, .content, .article-content, .news-content, .text"
                }
            },
            {
                "name": "ä¸­å±±å¸‚ä¸­çº§äººæ°‘æ³•é™¢",
                "url": "http://www.zscourt.gov.cn/",
                "region": "ä¸­å±±",
                "selectors": {
                    "title": "h1, h2, h3, .title, .news-title, .article-title",
                    "content": "p, .content, .article-content, .news-content, .text"
                }
            },
            {
                "name": "æƒ å·å¸‚ä¸­çº§äººæ°‘æ³•é™¢",
                "url": "http://www.hzcourt.gov.cn/",
                "region": "æƒ å·",
                "selectors": {
                    "title": "h1, h2, h3, .title, .news-title, .article-title",
                    "content": "p, .content, .article-content, .news-content, .text"
                }
            },
            {
                "name": "æ±Ÿé—¨å¸‚ä¸­çº§äººæ°‘æ³•é™¢",
                "url": "http://www.jmcourt.gov.cn/",
                "region": "æ±Ÿé—¨",
                "selectors": {
                    "title": "h1, h2, h3, .title, .news-title, .article-title",
                    "content": "p, .content, .article-content, .news-content, .text"
                }
            },
            {
                "name": "è‚‡åº†å¸‚ä¸­çº§äººæ°‘æ³•é™¢",
                "url": "http://www.zqcourt.gov.cn/",
                "region": "è‚‡åº†",
                "selectors": {
                    "title": "h1, h2, h3, .title, .news-title, .article-title",
                    "content": "p, .content, .article-content, .news-content, .text"
                }
            }
        ]
        
        for court in gba_courts:
            try:
                logger.info(f"ğŸ“¡ æ­£åœ¨æ”¶é›†: {court['name']} ({court['region']})")
                self._scrape_court_website(court)
                time.sleep(3)  # é¿å…è¯·æ±‚è¿‡å¿«
            except Exception as e:
                logger.error(f"æ”¶é›†å¤±è´¥ {court['name']}: {e}")
    
    def _scrape_court_website(self, court_config):
        """çˆ¬å–å•ä¸ªæ³•é™¢ç½‘ç«™"""
        try:
            response = requests.get(court_config['url'], timeout=15, headers=self.headers)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–æ ‡é¢˜å’Œå†…å®¹
            titles = soup.select(court_config['selectors']['title'])
            contents = soup.select(court_config['selectors']['content'])
            
            for title, content in zip(titles, contents):
                if title.text.strip() and content.text.strip():
                    # ä½¿ç”¨DISC-LawLLMçš„åˆ†ç±»æ–¹æ³•
                    category, confidence = self._classify_legal_text(title.text.strip())
                    
                    data = {
                        "text": f"{title.text.strip()} {content.text.strip()}",
                        "title": title.text.strip(),
                        "content": content.text.strip(),
                        "source": court_config['name'],
                        "region": court_config['region'],
                        "category": category,
                        "confidence": confidence,
                        "timestamp": datetime.now().isoformat(),
                        "url": court_config['url'],
                        "word_count": len(title.text.strip() + content.text.strip()),
                        "keywords": self._extract_keywords(title.text.strip() + content.text.strip())
                    }
                    self.collected_data.append(data)
                    logger.info(f"âœ… æ”¶é›†åˆ°: {title.text.strip()[:50]}... (ç±»åˆ«: {category})")
                    
        except Exception as e:
            logger.error(f"çˆ¬å–å¤±è´¥ {court_config['name']}: {e}")
    
    def _classify_legal_text(self, text: str) -> Tuple[str, float]:
        """ä½¿ç”¨DISC-LawLLMæ–¹æ³•åˆ†ç±»æ³•å¾‹æ–‡æœ¬"""
        text_lower = text.lower()
        category_scores = {}
        
        for category, info in self.legal_categories.items():
            score = 0
            keywords = info['keywords']
            
            # è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
            for keyword in keywords:
                if keyword in text_lower:
                    score += 1
            
            # è®¡ç®—å­ç±»åˆ«åŒ¹é…åˆ†æ•°
            for subcategory in info['subcategories']:
                if subcategory in text_lower:
                    score += 2
            
            category_scores[category] = score
        
        # æ‰¾åˆ°æœ€é«˜åˆ†çš„ç±»åˆ«
        best_category = max(category_scores, key=category_scores.get)
        max_score = category_scores[best_category]
        
        # è®¡ç®—ç½®ä¿¡åº¦
        total_score = sum(category_scores.values())
        confidence = max_score / total_score if total_score > 0 else 0.0
        
        return best_category, confidence
    
    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # ä½¿ç”¨jiebaåˆ†è¯
        words = jieba.lcut(text)
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'}
        keywords = [word for word in words if len(word) > 1 and word not in stop_words]
        
        # è¿”å›æœ€å¸¸è§çš„10ä¸ªå…³é”®è¯
        return [word for word, count in Counter(keywords).most_common(10)]
    
    def collect_legal_news(self):
        """æ”¶é›†æ³•å¾‹æ–°é—»æ•°æ®"""
        logger.info("ğŸ“° å¼€å§‹æ”¶é›†æ³•å¾‹æ–°é—»æ•°æ®...")
        
        # æ³•å¾‹æ–°é—»ç½‘ç«™
        news_sites = [
            {
                "name": "ä¸­å›½æ³•é™¢ç½‘",
                "url": "http://www.chinacourt.org/",
                "selectors": {
                    "title": "h1, h2, h3, .title, .news-title, .article-title",
                    "content": "p, .content, .article-content, .news-content, .text"
                }
            },
            {
                "name": "æ³•å¾‹å›¾ä¹¦é¦†",
                "url": "http://www.law-lib.com/",
                "selectors": {
                    "title": "h1, h2, h3, .title, .news-title, .article-title",
                    "content": "p, .content, .article-content, .news-content, .text"
                }
            },
            {
                "name": "ä¸­å›½æ³•å¾‹ç½‘",
                "url": "http://www.chinalaw.com/",
                "selectors": {
                    "title": "h1, h2, h3, .title, .news-title, .article-title",
                    "content": "p, .content, .article-content, .news-content, .text"
                }
            }
        ]
        
        for site in news_sites:
            try:
                logger.info(f"ğŸ“¡ æ­£åœ¨æ”¶é›†: {site['name']}")
                self._scrape_news_website(site)
                time.sleep(2)
            except Exception as e:
                logger.error(f"æ”¶é›†å¤±è´¥ {site['name']}: {e}")
    
    def _scrape_news_website(self, site_config):
        """çˆ¬å–æ–°é—»ç½‘ç«™"""
        try:
            response = requests.get(site_config['url'], timeout=15, headers=self.headers)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–æ ‡é¢˜å’Œå†…å®¹
            titles = soup.select(site_config['selectors']['title'])
            contents = soup.select(site_config['selectors']['content'])
            
            for title, content in zip(titles, contents):
                if title.text.strip() and content.text.strip():
                    # ä½¿ç”¨DISC-LawLLMçš„åˆ†ç±»æ–¹æ³•
                    category, confidence = self._classify_legal_text(title.text.strip())
                    
                    data = {
                        "text": f"{title.text.strip()} {content.text.strip()}",
                        "title": title.text.strip(),
                        "content": content.text.strip(),
                        "source": site_config['name'],
                        "region": "ç²¤æ¸¯æ¾³å¤§æ¹¾åŒº",
                        "category": category,
                        "confidence": confidence,
                        "timestamp": datetime.now().isoformat(),
                        "url": site_config['url'],
                        "word_count": len(title.text.strip() + content.text.strip()),
                        "keywords": self._extract_keywords(title.text.strip() + content.text.strip())
                    }
                    self.collected_data.append(data)
                    logger.info(f"âœ… æ”¶é›†åˆ°æ–°é—»: {title.text.strip()[:50]}... (ç±»åˆ«: {category})")
                    
        except Exception as e:
            logger.error(f"çˆ¬å–å¤±è´¥ {site_config['name']}: {e}")
    
    def create_training_data(self):
        """åˆ›å»ºè®­ç»ƒæ•°æ®"""
        logger.info("ğŸ“š åˆ›å»ºDISC-LawLLMè®­ç»ƒæ•°æ®...")
        
        # åˆ›å»ºè®­ç»ƒæ•°æ®
        training_data = []
        for item in self.collected_data:
            training_data.append({
                "text": item['text'],
                "label": self._get_category_id(item['category']),
                "category": item['category'],
                "confidence": item['confidence'],
                "source": item['source'],
                "region": item['region'],
                "keywords": item['keywords']
            })
        
        # ä¿å­˜è®­ç»ƒæ•°æ®
        training_file = os.path.join(self.output_dir, "disc_law_training_data.json")
        with open(training_file, 'w', encoding='utf-8') as f:
            json.dump(training_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {training_file}")
        return training_data
    
    def _get_category_id(self, category: str) -> int:
        """è·å–ç±»åˆ«ID"""
        category_list = list(self.legal_categories.keys())
        return category_list.index(category) if category in category_list else len(category_list) - 1
    
    def save_data(self):
        """ä¿å­˜æ”¶é›†çš„æ•°æ®"""
        logger.info("ğŸ’¾ ä¿å­˜æ”¶é›†çš„æ•°æ®...")
        
        # ä¿å­˜ä¸ºJSONæ ¼å¼
        json_file = os.path.join(self.output_dir, "disc_law_data.json")
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.collected_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ä¸ºCSVæ ¼å¼
        df = pd.DataFrame(self.collected_data)
        csv_file = os.path.join(self.output_dir, "disc_law_data.csv")
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        
        # ä¿å­˜ä¸ºTXTæ ¼å¼ï¼ˆç”¨äºBERTè®­ç»ƒï¼‰
        txt_file = os.path.join(self.output_dir, "disc_law_data.txt")
        with open(txt_file, 'w', encoding='utf-8') as f:
            for item in self.collected_data:
                f.write(f"{item['text']}\n")
        
        logger.info(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {self.output_dir}")
        logger.info(f"ğŸ“Š æ€»å…±æ”¶é›†äº† {len(self.collected_data)} æ¡æ•°æ®")
        
        # æŒ‰åœ°åŒºç»Ÿè®¡
        region_stats = {}
        for item in self.collected_data:
            region = item.get('region', 'æœªçŸ¥')
            region_stats[region] = region_stats.get(region, 0) + 1
        
        logger.info("\nğŸ“ˆ æŒ‰åœ°åŒºç»Ÿè®¡:")
        for region, count in region_stats.items():
            logger.info(f"  {region}: {count} æ¡")
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        category_stats = {}
        for item in self.collected_data:
            category = item.get('category', 'æœªçŸ¥')
            category_stats[category] = category_stats.get(category, 0) + 1
        
        logger.info("\nğŸ“ˆ æŒ‰ç±»åˆ«ç»Ÿè®¡:")
        for category, count in category_stats.items():
            logger.info(f"  {category}: {count} æ¡")
        
        # åˆ›å»ºè®­ç»ƒæ•°æ®
        training_data = self.create_training_data()
        logger.info(f"ğŸ“š åˆ›å»ºäº† {len(training_data)} æ¡è®­ç»ƒæ•°æ®")

async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸŒ åŸºäºDISC-LawLLMçš„ç²¤æ¸¯æ¾³å¤§æ¹¾åŒºæ³•å¾‹æ•°æ®æ”¶é›†å™¨")
    logger.info("=" * 60)
    
    collector = DISCLawDataCollector()
    
    try:
        # 1. æ”¶é›†æ³•é™¢æ•°æ®
        logger.info("\n1. æ”¶é›†ç²¤æ¸¯æ¾³å¤§æ¹¾åŒºæ³•é™¢æ•°æ®...")
        collector.collect_gba_court_data()
        
        # 2. æ”¶é›†æ³•å¾‹æ–°é—»
        logger.info("\n2. æ”¶é›†æ³•å¾‹æ–°é—»...")
        collector.collect_legal_news()
        
        # 3. ä¿å­˜æ•°æ®
        logger.info("\n3. ä¿å­˜æ•°æ®...")
        collector.save_data()
        
        logger.info("\nğŸ‰ æ•°æ®æ”¶é›†å®Œæˆ!")
        logger.info(f"ğŸ“ æ•°æ®ä¿å­˜åœ¨: {collector.output_dir}")
        
    except Exception as e:
        logger.error(f"æ•°æ®æ”¶é›†å¤±è´¥: {e}")
    finally:
        logger.info("\nğŸ“Š æ”¶é›†ç»Ÿè®¡:")
        logger.info(f"æ€»æ•°æ®é‡: {len(collector.collected_data)} æ¡")

if __name__ == "__main__":
    asyncio.run(main())






