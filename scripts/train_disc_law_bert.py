#!/usr/bin/env python3
"""
åŸºäºDISC-LawLLMæ¶æ„çš„BERTæ¨¡å‹è®­ç»ƒè„šæœ¬
å‚è€ƒ: https://github.com/FudanDISC/DISC-LawLLM
"""
import os
import sys
import asyncio
import argparse
import logging
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from backend.app.services.disc_law_bert import DISCLawBERT, GBALegalDataProcessor

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è®­ç»ƒåŸºäºDISC-LawLLMæ¶æ„çš„BERTæ¨¡å‹")
    parser.add_argument("--data_dir", type=str, default="data/disc_law_data",
                        help="æ•°æ®ç›®å½•è·¯å¾„")
    parser.add_argument("--model_name", type=str, default="bert-base-chinese",
                        help="é¢„è®­ç»ƒæ¨¡å‹åç§°")
    parser.add_argument("--output_dir", type=str, default="./models/disc_law_bert",
                        help="æ¨¡å‹è¾“å‡ºç›®å½•")
    parser.add_argument("--epochs", type=int, default=5,
                        help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=16,
                        help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--learning_rate", type=float, default=2e-5,
                        help="å­¦ä¹ ç‡")
    parser.add_argument("--max_length", type=int, default=512,
                        help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--warmup_steps", type=int, default=500,
                        help="é¢„çƒ­æ­¥æ•°")
    parser.add_argument("--test_size", type=float, default=0.2,
                        help="æµ‹è¯•é›†æ¯”ä¾‹")
    
    args = parser.parse_args()
    
    logger.info("ğŸŒ åŸºäºDISC-LawLLMæ¶æ„çš„BERTæ¨¡å‹è®­ç»ƒ")
    logger.info("=" * 60)
    logger.info(f"æ•°æ®ç›®å½•: {args.data_dir}")
    logger.info(f"æ¨¡å‹åç§°: {args.model_name}")
    logger.info(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    logger.info(f"è®­ç»ƒè½®æ•°: {args.epochs}")
    logger.info(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    logger.info(f"å­¦ä¹ ç‡: {args.learning_rate}")
    
    try:
        # 1. åˆå§‹åŒ–æ¨¡å‹é…ç½®
        logger.info("\n1. åˆå§‹åŒ–DISC-LawLLM BERTæ¨¡å‹é…ç½®...")
        config = {
            'model_name': args.model_name,
            'num_labels': 10,  # 10ä¸ªæ³•å¾‹ç±»åˆ«
            'max_length': args.max_length,
            'epochs': args.epochs,
            'batch_size': args.batch_size,
            'learning_rate': args.learning_rate,
            'warmup_steps': args.warmup_steps,
            'output_dir': args.output_dir
        }
        
        # 2. åˆå§‹åŒ–æ¨¡å‹
        logger.info("\n2. åˆå§‹åŒ–DISC-LawLLM BERTæ¨¡å‹...")
        model = DISCLawBERT(config)
        
        # 3. åŠ è½½æ•°æ®
        logger.info("\n3. åŠ è½½ç²¤æ¸¯æ¾³å¤§æ¹¾åŒºæ³•å¾‹æ•°æ®...")
        data_processor = GBALegalDataProcessor(args.data_dir)
        texts, labels = data_processor.load_data()
        
        if not texts:
            logger.error("æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®æ”¶é›†è„šæœ¬")
            return
        
        logger.info(f"åŠ è½½äº† {len(texts)} æ¡è®­ç»ƒæ•°æ®")
        
        # 4. å‡†å¤‡æ•°æ®é›†
        logger.info("\n4. å‡†å¤‡æ•°æ®é›†...")
        dataset_dict = model.prepare_dataset(texts, labels, test_size=args.test_size)
        
        # 5. è®­ç»ƒæ¨¡å‹
        logger.info("\n5. å¼€å§‹è®­ç»ƒDISC-LawLLM BERTæ¨¡å‹...")
        start_time = datetime.now()
        metrics = model.train(dataset_dict)
        end_time = datetime.now()
        
        training_time = (end_time - start_time).total_seconds()
        logger.info(f"è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f} ç§’")
        
        # 6. ä¿å­˜æ¨¡å‹
        logger.info("\n6. ä¿å­˜æ¨¡å‹...")
        model.save_model(args.output_dir)
        
        # 7. æ˜¾ç¤ºè®­ç»ƒç»“æœ
        logger.info("\nğŸ‰ DISC-LawLLM BERTæ¨¡å‹è®­ç»ƒå®Œæˆ!")
        logger.info(f"ğŸ“Š è®­ç»ƒæŒ‡æ ‡:")
        for metric, value in metrics.items():
            logger.info(f"  {metric}: {value:.4f}")
        
        logger.info(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {args.output_dir}")
        logger.info(f"â±ï¸ è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
        
        # 8. åˆ›å»ºæ¨¡å‹ä½¿ç”¨è¯´æ˜
        usage_file = os.path.join(args.output_dir, "README.md")
        with open(usage_file, 'w', encoding='utf-8') as f:
            f.write(f"""# DISC-LawLLM BERTæ¨¡å‹

## æ¨¡å‹ä¿¡æ¯
- æ¨¡å‹ç±»å‹: åŸºäºDISC-LawLLMæ¶æ„çš„æ³•å¾‹BERTæ¨¡å‹
- é¢„è®­ç»ƒæ¨¡å‹: {args.model_name}
- è®­ç»ƒæ•°æ®: ç²¤æ¸¯æ¾³å¤§æ¹¾åŒºæ³•å¾‹æ•°æ®
- ç±»åˆ«æ•°é‡: 10ä¸ªæ³•å¾‹ç±»åˆ«
- è®­ç»ƒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## è®­ç»ƒæŒ‡æ ‡
""")
            for metric, value in metrics.items():
                f.write(f"- {metric}: {value:.4f}\n")
            
            f.write(f"""
## ä½¿ç”¨æ–¹æ³•

```python
from backend.app.services.disc_law_bert import DISCLawBERT

# åŠ è½½æ¨¡å‹
model = DISCLawBERT.load_model('{args.output_dir}')

# é¢„æµ‹æ–‡æœ¬ç±»åˆ«
texts = ["æ ¹æ®åˆåŒæ³•è§„å®š...", "åˆ‘æ³•ä¿®æ­£æ¡ˆ..."]
results = model.predict(texts)

for result in results:
    print(f"æ–‡æœ¬: {result['text']}")
    print(f"é¢„æµ‹ç±»åˆ«: {result['predicted_label']}")
    print(f"ç½®ä¿¡åº¦: {result['confidence']:.4f}")
```

## æ³•å¾‹ç±»åˆ«
1. æ°‘äº‹
2. åˆ‘äº‹
3. è¡Œæ”¿
4. å•†äº‹
5. åŠ³åŠ¨
6. çŸ¥è¯†äº§æƒ
7. ç¯å¢ƒ
8. å›½é™…
9. é‡‘è
10. å…¶ä»–
""")
        
        logger.info(f"ğŸ“– ä½¿ç”¨è¯´æ˜å·²ä¿å­˜åˆ°: {usage_file}")
        
    except Exception as e:
        logger.error(f"è®­ç»ƒå¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(main())






