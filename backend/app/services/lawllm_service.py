#!/usr/bin/env python3
"""
LawLLM-7B æ³•å¾‹æœåŠ¡
åŸºäº ShengbinYue/LawLLM-7B æ¨¡å‹çš„æ³•å¾‹æ™ºèƒ½æœåŠ¡
"""
import os
import sys
import logging
from typing import List, Dict, Any, Optional
import torch
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams
import json
from datetime import datetime

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class LawLLMService:
    """LawLLM-7B æ³•å¾‹æœåŠ¡ç±»"""
    
    def __init__(self, model_name: str = "ShengbinYue/LawLLM-7B"):
        self.model_name = model_name
        self.llm = None
        self.tokenizer = None
        self.sampling_params = None
        self.is_initialized = False
        
    def initialize(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        try:
            logger.info(f"ğŸš€ åˆå§‹åŒ– LawLLM-7B æ¨¡å‹: {self.model_name}")
            
            # è®¾ç½®é‡‡æ ·å‚æ•°
            self.sampling_params = SamplingParams(
                temperature=0.1,
                top_p=0.9,
                top_k=50,
                max_tokens=4096
            )
            
            # åˆå§‹åŒ– LLM
            self.llm = LLM(model=self.model_name)
            
            # åˆå§‹åŒ–åˆ†è¯å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            self.is_initialized = True
            logger.info("âœ… LawLLM-7B æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def generate_response(self, prompt: str, system_message: str = None) -> str:
        """ç”Ÿæˆæ³•å¾‹å’¨è¯¢å›å¤"""
        if not self.is_initialized:
            self.initialize()
        
        try:
            # æ„å»ºæ¶ˆæ¯
            if system_message is None:
                system_message = "ä½ æ˜¯LawLLMï¼Œä¸€ä¸ªç”±å¤æ—¦å¤§å­¦DISCå®éªŒå®¤åˆ›é€ çš„æ³•å¾‹åŠ©æ‰‹ã€‚"
            
            messages = [
                {"role": "system", "content": system_message},
                {"role": "user", "content": prompt}
            ]
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # ç”Ÿæˆå›å¤
            outputs = self.llm.generate([text], self.sampling_params)
            
            for output in outputs:
                generated_text = output.outputs[0].text
                return generated_text.strip()
                
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå›å¤å¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆå›å¤æ—¶å‡ºç°é”™è¯¯: {str(e)}"
    
    def legal_consultation(self, question: str, context: str = None) -> Dict[str, Any]:
        """æ³•å¾‹å’¨è¯¢"""
        try:
            # æ„å»ºæç¤ºè¯
            if context:
                prompt = f"é—®é¢˜: {question}\n\nèƒŒæ™¯ä¿¡æ¯: {context}\n\nè¯·æä¾›è¯¦ç»†çš„æ³•å¾‹åˆ†æå’Œå»ºè®®ã€‚"
            else:
                prompt = question
            
            # ç”Ÿæˆå›å¤
            response = self.generate_response(prompt)
            
            # åˆ†æå›å¤è´¨é‡
            confidence = self._analyze_response_quality(response)
            
            return {
                "question": question,
                "answer": response,
                "confidence": confidence,
                "model": self.model_name,
                "timestamp": datetime.now().isoformat(),
                "context": context
            }
            
        except Exception as e:
            logger.error(f"âŒ æ³•å¾‹å’¨è¯¢å¤±è´¥: {e}")
            return {
                "question": question,
                "answer": f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                "confidence": 0.0,
                "model": self.model_name,
                "timestamp": datetime.now().isoformat(),
                "context": context
            }
    
    def legal_analysis(self, case_text: str) -> Dict[str, Any]:
        """æ³•å¾‹æ¡ˆä¾‹åˆ†æ"""
        try:
            prompt = f"è¯·åˆ†æä»¥ä¸‹æ³•å¾‹æ¡ˆä¾‹ï¼ŒåŒ…æ‹¬æ¡ˆä»¶æ€§è´¨ã€é€‚ç”¨æ³•å¾‹ã€å¯èƒ½çš„æ³•å¾‹åæœç­‰:\n\n{case_text}"
            
            response = self.generate_response(prompt)
            confidence = self._analyze_response_quality(response)
            
            return {
                "case_text": case_text,
                "analysis": response,
                "confidence": confidence,
                "model": self.model_name,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ æ³•å¾‹åˆ†æå¤±è´¥: {e}")
            return {
                "case_text": case_text,
                "analysis": f"æŠ±æ­‰ï¼Œåˆ†ææ¡ˆä¾‹æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                "confidence": 0.0,
                "model": self.model_name,
                "timestamp": datetime.now().isoformat()
            }
    
    def legal_document_review(self, document_text: str) -> Dict[str, Any]:
        """æ³•å¾‹æ–‡æ¡£å®¡æŸ¥"""
        try:
            prompt = f"è¯·å®¡æŸ¥ä»¥ä¸‹æ³•å¾‹æ–‡æ¡£ï¼ŒæŒ‡å‡ºæ½œåœ¨çš„æ³•å¾‹é£é™©ã€åˆè§„é—®é¢˜å’Œæ”¹è¿›å»ºè®®:\n\n{document_text}"
            
            response = self.generate_response(prompt)
            confidence = self._analyze_response_quality(response)
            
            return {
                "document_text": document_text,
                "review": response,
                "confidence": confidence,
                "model": self.model_name,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ æ³•å¾‹æ–‡æ¡£å®¡æŸ¥å¤±è´¥: {e}")
            return {
                "document_text": document_text,
                "review": f"æŠ±æ­‰ï¼Œå®¡æŸ¥æ–‡æ¡£æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                "confidence": 0.0,
                "model": self.model_name,
                "timestamp": datetime.now().isoformat()
            }
    
    def legal_research(self, research_topic: str) -> Dict[str, Any]:
        """æ³•å¾‹ç ”ç©¶"""
        try:
            prompt = f"è¯·å°±ä»¥ä¸‹æ³•å¾‹ç ”ç©¶ä¸»é¢˜æä¾›è¯¦ç»†çš„ç ”ç©¶æŠ¥å‘Šï¼ŒåŒ…æ‹¬ç›¸å…³æ³•å¾‹æ¡æ–‡ã€æ¡ˆä¾‹åˆ†æã€å­¦æœ¯è§‚ç‚¹ç­‰:\n\n{research_topic}"
            
            response = self.generate_response(prompt)
            confidence = self._analyze_response_quality(response)
            
            return {
                "research_topic": research_topic,
                "research_report": response,
                "confidence": confidence,
                "model": self.model_name,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ æ³•å¾‹ç ”ç©¶å¤±è´¥: {e}")
            return {
                "research_topic": research_topic,
                "research_report": f"æŠ±æ­‰ï¼Œè¿›è¡Œç ”ç©¶æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                "confidence": 0.0,
                "model": self.model_name,
                "timestamp": datetime.now().isoformat()
            }
    
    def _analyze_response_quality(self, response: str) -> float:
        """åˆ†æå›å¤è´¨é‡"""
        try:
            # ç®€å•çš„è´¨é‡è¯„ä¼°æŒ‡æ ‡
            quality_score = 0.0
            
            # é•¿åº¦æ£€æŸ¥
            if len(response) > 50:
                quality_score += 0.2
            
            # æ³•å¾‹å…³é”®è¯æ£€æŸ¥
            legal_keywords = ["æ³•å¾‹", "æ³•è§„", "æ¡æ–‡", "è§„å®š", "æ¡æ¬¾", "æ¡ˆä¾‹", "åˆ¤å†³", "æ³•é™¢", "å¾‹å¸ˆ", "è¯‰è®¼"]
            keyword_count = sum(1 for keyword in legal_keywords if keyword in response)
            quality_score += min(keyword_count * 0.1, 0.4)
            
            # ç»“æ„æ£€æŸ¥
            if "ã€‚" in response and "ï¼Œ" in response:
                quality_score += 0.2
            
            # å®Œæ•´æ€§æ£€æŸ¥
            if len(response.split()) > 20:
                quality_score += 0.2
            
            return min(quality_score, 1.0)
            
        except Exception as e:
            logger.error(f"âŒ è´¨é‡åˆ†æå¤±è´¥: {e}")
            return 0.5
    
    def batch_consultation(self, questions: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡æ³•å¾‹å’¨è¯¢"""
        results = []
        
        for question in questions:
            try:
                result = self.legal_consultation(question)
                results.append(result)
            except Exception as e:
                logger.error(f"âŒ æ‰¹é‡å’¨è¯¢å¤±è´¥: {e}")
                results.append({
                    "question": question,
                    "answer": f"å¤„ç†å¤±è´¥: {str(e)}",
                    "confidence": 0.0,
                    "model": self.model_name,
                    "timestamp": datetime.now().isoformat()
                })
        
        return results
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "model_name": self.model_name,
            "is_initialized": self.is_initialized,
            "sampling_params": {
                "temperature": self.sampling_params.temperature if self.sampling_params else None,
                "top_p": self.sampling_params.top_p if self.sampling_params else None,
                "top_k": self.sampling_params.top_k if self.sampling_params else None,
                "max_tokens": self.sampling_params.max_tokens if self.sampling_params else None
            },
            "timestamp": datetime.now().isoformat()
        }

# å…¨å±€æœåŠ¡å®ä¾‹
lawllm_service = LawLLMService()

def get_lawllm_service() -> LawLLMService:
    """è·å– LawLLM æœåŠ¡å®ä¾‹"""
    return lawllm_service






