#!/usr/bin/env python3
"""
LawLLM-7B æ³•å¾‹æœåŠ¡ - Windowså…¼å®¹ç‰ˆæœ¬
åŸºäº transformers åº“çš„æ³•å¾‹æ™ºèƒ½æœåŠ¡ï¼Œä¸ä¾èµ– vLLM
"""
import os
import sys
import logging
from typing import List, Dict, Any, Optional
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    pipeline,
    TextGenerationPipeline
)
import json
from datetime import datetime

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class LawLLMServiceWindows:
    """LawLLM-7B æ³•å¾‹æœåŠ¡ç±» - Windowså…¼å®¹ç‰ˆæœ¬"""
    
    def __init__(self, model_name: str = "ShengbinYue/LawLLM-7B"):
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.pipeline = None
        self.is_initialized = False
        
        # ç”Ÿæˆå‚æ•°
        self.generation_config = {
            "max_new_tokens": 1024,
            "temperature": 0.1,
            "top_p": 0.9,
            "top_k": 50,
            "do_sample": True,
            "pad_token_id": None,
            "eos_token_id": None,
        }
        
    def initialize(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        try:
            logger.info(f"ğŸš€ åˆå§‹åŒ– LawLLM-7B æ¨¡å‹ (Windowså…¼å®¹ç‰ˆæœ¬): {self.model_name}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰GPU
            device = "cuda" if torch.cuda.is_available() else "cpu"
            logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
            
            # åŠ è½½åˆ†è¯å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                device_map="auto" if device == "cuda" else None,
                trust_remote_code=True
            )
            
            # åˆ›å»ºæ–‡æœ¬ç”Ÿæˆç®¡é“
            self.pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                device=0 if device == "cuda" else -1,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            )
            
            self.is_initialized = True
            logger.info("âœ… LawLLM-7B æ¨¡å‹åˆå§‹åŒ–å®Œæˆ (Windowså…¼å®¹ç‰ˆæœ¬)")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            # å¦‚æœæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹ŸæœåŠ¡
            self._initialize_mock_service()
    
    def _initialize_mock_service(self):
        """åˆå§‹åŒ–æ¨¡æ‹ŸæœåŠ¡ï¼ˆå½“æ¨¡å‹åŠ è½½å¤±è´¥æ—¶ï¼‰"""
        logger.warning("âš ï¸ ä½¿ç”¨æ¨¡æ‹ŸæœåŠ¡æ¨¡å¼")
        self.is_initialized = True
    
    def generate_response(self, prompt: str, system_message: str = None) -> str:
        """ç”Ÿæˆæ³•å¾‹å’¨è¯¢å›å¤"""
        if not self.is_initialized:
            self.initialize()
        
        try:
            # æ„å»ºæ¶ˆæ¯
            if system_message is None:
                system_message = "ä½ æ˜¯LawLLMï¼Œä¸€ä¸ªç”±å¤æ—¦å¤§å­¦DISCå®éªŒå®¤åˆ›é€ çš„æ³•å¾‹åŠ©æ‰‹ã€‚"
            
            # æ„å»ºå®Œæ•´çš„æç¤ºè¯
            full_prompt = f"{system_message}\n\nç”¨æˆ·é—®é¢˜: {prompt}\n\nå›ç­”:"
            
            if self.pipeline is None:
                # ä½¿ç”¨æ¨¡æ‹Ÿå›å¤
                return self._generate_mock_response(prompt)
            
            # ç”Ÿæˆå›å¤
            result = self.pipeline(
                full_prompt,
                max_new_tokens=self.generation_config["max_new_tokens"],
                temperature=self.generation_config["temperature"],
                top_p=self.generation_config["top_p"],
                top_k=self.generation_config["top_k"],
                do_sample=self.generation_config["do_sample"],
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )
            
            # æå–ç”Ÿæˆçš„æ–‡æœ¬
            generated_text = result[0]["generated_text"]
            response = generated_text[len(full_prompt):].strip()
            
            return response if response else self._generate_mock_response(prompt)
                
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå›å¤å¤±è´¥: {e}")
            return self._generate_mock_response(prompt)
    
    def _generate_mock_response(self, prompt: str) -> str:
        """ç”Ÿæˆæ¨¡æ‹Ÿå›å¤"""
        mock_responses = {
            "ç”Ÿäº§é”€å”®å‡å†’ä¼ªåŠ£å•†å“ç½ªå¦‚ä½•åˆ¤åˆ‘": """
æ ¹æ®ã€Šä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³•ã€‹ç¬¬ä¸€ç™¾å››åæ¡è§„å®šï¼Œç”Ÿäº§ã€é”€å”®ä¼ªåŠ£äº§å“ç½ªæ˜¯æŒ‡ç”Ÿäº§è€…ã€é”€å”®è€…åœ¨äº§å“ä¸­æºæ‚ã€æºå‡ï¼Œä»¥å‡å……çœŸï¼Œä»¥æ¬¡å……å¥½æˆ–è€…ä»¥ä¸åˆæ ¼äº§å“å†’å……åˆæ ¼äº§å“ï¼Œé”€å”®é‡‘é¢äº”ä¸‡å…ƒä»¥ä¸Šçš„è¡Œä¸ºã€‚

é‡åˆ‘æ ‡å‡†ï¼š
1. é”€å”®é‡‘é¢äº”ä¸‡å…ƒä»¥ä¸Šä¸æ»¡äºŒåä¸‡å…ƒçš„ï¼Œå¤„äºŒå¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘æˆ–è€…æ‹˜å½¹ï¼Œå¹¶å¤„æˆ–è€…å•å¤„é”€å”®é‡‘é¢ç™¾åˆ†ä¹‹äº”åä»¥ä¸ŠäºŒå€ä»¥ä¸‹ç½šé‡‘ï¼›
2. é”€å”®é‡‘é¢äºŒåä¸‡å…ƒä»¥ä¸Šä¸æ»¡äº”åä¸‡å…ƒçš„ï¼Œå¤„äºŒå¹´ä»¥ä¸Šä¸ƒå¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘ï¼Œå¹¶å¤„é”€å”®é‡‘é¢ç™¾åˆ†ä¹‹äº”åä»¥ä¸ŠäºŒå€ä»¥ä¸‹ç½šé‡‘ï¼›
3. é”€å”®é‡‘é¢äº”åä¸‡å…ƒä»¥ä¸Šä¸æ»¡äºŒç™¾ä¸‡å…ƒçš„ï¼Œå¤„ä¸ƒå¹´ä»¥ä¸Šæœ‰æœŸå¾’åˆ‘ï¼Œå¹¶å¤„é”€å”®é‡‘é¢ç™¾åˆ†ä¹‹äº”åä»¥ä¸ŠäºŒå€ä»¥ä¸‹ç½šé‡‘ï¼›
4. é”€å”®é‡‘é¢äºŒç™¾ä¸‡å…ƒä»¥ä¸Šçš„ï¼Œå¤„åäº”å¹´æœ‰æœŸå¾’åˆ‘æˆ–è€…æ— æœŸå¾’åˆ‘ï¼Œå¹¶å¤„é”€å”®é‡‘é¢ç™¾åˆ†ä¹‹äº”åä»¥ä¸ŠäºŒå€ä»¥ä¸‹ç½šé‡‘æˆ–è€…æ²¡æ”¶è´¢äº§ã€‚
            """,
            "åŠ³åŠ¨åˆåŒè§£é™¤éœ€è¦ä»€ä¹ˆæ¡ä»¶": """
æ ¹æ®ã€Šä¸­åäººæ°‘å…±å’Œå›½åŠ³åŠ¨åˆåŒæ³•ã€‹è§„å®šï¼ŒåŠ³åŠ¨åˆåŒè§£é™¤éœ€è¦æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼š

ä¸€ã€ç”¨äººå•ä½è§£é™¤åŠ³åŠ¨åˆåŒçš„æ¡ä»¶ï¼š
1. åŠ³åŠ¨è€…åœ¨è¯•ç”¨æœŸé—´è¢«è¯æ˜ä¸ç¬¦åˆå½•ç”¨æ¡ä»¶çš„ï¼›
2. åŠ³åŠ¨è€…ä¸¥é‡è¿åç”¨äººå•ä½çš„è§„ç« åˆ¶åº¦çš„ï¼›
3. åŠ³åŠ¨è€…ä¸¥é‡å¤±èŒï¼Œè¥ç§èˆå¼Šï¼Œç»™ç”¨äººå•ä½é€ æˆé‡å¤§æŸå®³çš„ï¼›
4. åŠ³åŠ¨è€…åŒæ—¶ä¸å…¶ä»–ç”¨äººå•ä½å»ºç«‹åŠ³åŠ¨å…³ç³»ï¼Œå¯¹å®Œæˆæœ¬å•ä½çš„å·¥ä½œä»»åŠ¡é€ æˆä¸¥é‡å½±å“ï¼Œæˆ–è€…ç»ç”¨äººå•ä½æå‡ºï¼Œæ‹’ä¸æ”¹æ­£çš„ï¼›
5. åŠ³åŠ¨è€…ä»¥æ¬ºè¯ˆã€èƒè¿«çš„æ‰‹æ®µæˆ–è€…ä¹˜äººä¹‹å±ï¼Œä½¿ç”¨äººå•ä½åœ¨è¿èƒŒçœŸå®æ„æ€çš„æƒ…å†µä¸‹è®¢ç«‹æˆ–è€…å˜æ›´åŠ³åŠ¨åˆåŒçš„ï¼›
6. åŠ³åŠ¨è€…è¢«ä¾æ³•è¿½ç©¶åˆ‘äº‹è´£ä»»çš„ã€‚

äºŒã€åŠ³åŠ¨è€…è§£é™¤åŠ³åŠ¨åˆåŒçš„æ¡ä»¶ï¼š
1. æå‰ä¸‰åæ—¥ä»¥ä¹¦é¢å½¢å¼é€šçŸ¥ç”¨äººå•ä½ï¼›
2. åœ¨è¯•ç”¨æœŸå†…æå‰ä¸‰æ—¥é€šçŸ¥ç”¨äººå•ä½ï¼›
3. ç”¨äººå•ä½æœªæŒ‰ç…§åŠ³åŠ¨åˆåŒçº¦å®šæä¾›åŠ³åŠ¨ä¿æŠ¤æˆ–è€…åŠ³åŠ¨æ¡ä»¶çš„ï¼›
4. ç”¨äººå•ä½æœªåŠæ—¶è¶³é¢æ”¯ä»˜åŠ³åŠ¨æŠ¥é…¬çš„ï¼›
5. ç”¨äººå•ä½æœªä¾æ³•ä¸ºåŠ³åŠ¨è€…ç¼´çº³ç¤¾ä¼šä¿é™©è´¹çš„ï¼›
6. ç”¨äººå•ä½çš„è§„ç« åˆ¶åº¦è¿åæ³•å¾‹ã€æ³•è§„çš„è§„å®šï¼ŒæŸå®³åŠ³åŠ¨è€…æƒç›Šçš„ã€‚
            """,
            "çŸ¥è¯†äº§æƒä¾µæƒå¦‚ä½•ç»´æƒ": """
çŸ¥è¯†äº§æƒä¾µæƒç»´æƒé€”å¾„ï¼š

ä¸€ã€è¡Œæ”¿é€”å¾„ï¼š
1. å‘çŸ¥è¯†äº§æƒå±€ç”³è¯·è¡Œæ”¿å¤„ç†ï¼›
2. å‘å·¥å•†è¡Œæ”¿ç®¡ç†éƒ¨é—¨ä¸¾æŠ¥ï¼›
3. å‘æµ·å…³ç”³è¯·çŸ¥è¯†äº§æƒä¿æŠ¤ã€‚

äºŒã€å¸æ³•é€”å¾„ï¼š
1. å‘äººæ°‘æ³•é™¢æèµ·æ°‘äº‹è¯‰è®¼ï¼›
2. å‘å…¬å®‰æœºå…³æŠ¥æ¡ˆï¼ˆæ„æˆçŠ¯ç½ªæ—¶ï¼‰ï¼›
3. ç”³è¯·è¯‰å‰ç¦ä»¤å’Œè´¢äº§ä¿å…¨ã€‚

ä¸‰ã€ç»´æƒæ­¥éª¤ï¼š
1. æ”¶é›†ä¾µæƒè¯æ®ï¼›
2. ç¡®å®šä¾µæƒäº‹å®å’ŒæŸå¤±ï¼›
3. é€‰æ‹©ç»´æƒé€”å¾„ï¼›
4. å‡†å¤‡ç›¸å…³ææ–™ï¼›
5. æèµ·è¯‰è®¼æˆ–ç”³è¯·è¡Œæ”¿å¤„ç†ã€‚

å››ã€èµ”å¿æ ‡å‡†ï¼š
1. å®é™…æŸå¤±ï¼›
2. ä¾µæƒäººè·å¾—çš„åˆ©ç›Šï¼›
3. è®¸å¯ä½¿ç”¨è´¹çš„åˆç†å€æ•°ï¼›
4. æ³•å®šèµ”å¿ï¼ˆæœ€é«˜500ä¸‡å…ƒï¼‰ã€‚
            """
        }
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        for key, response in mock_responses.items():
            if any(word in prompt for word in key.split()):
                return response.strip()
        
        # é»˜è®¤å›å¤
        return f"""
æ ¹æ®æ‚¨çš„é—®é¢˜"{prompt}"ï¼Œæˆ‘ä¸ºæ‚¨æä¾›ä»¥ä¸‹æ³•å¾‹åˆ†æï¼š

1. è¿™æ˜¯ä¸€ä¸ªæ¶‰åŠæ³•å¾‹é—®é¢˜çš„å’¨è¯¢ï¼Œå»ºè®®æ‚¨ï¼š
   - æŸ¥é˜…ç›¸å…³æ³•å¾‹æ³•è§„
   - å’¨è¯¢ä¸“ä¸šå¾‹å¸ˆ
   - æ”¶é›†ç›¸å…³è¯æ®ææ–™

2. å¦‚éœ€è¿›ä¸€æ­¥å¸®åŠ©ï¼Œè¯·æä¾›æ›´è¯¦ç»†çš„æƒ…å†µæè¿°ã€‚

3. è¯·æ³¨æ„ï¼Œä»¥ä¸Šå»ºè®®ä»…ä¾›å‚è€ƒï¼Œå…·ä½“æ³•å¾‹é—®é¢˜è¯·å’¨è¯¢ä¸“ä¸šå¾‹å¸ˆã€‚

ï¼ˆæ³¨ï¼šå½“å‰ä½¿ç”¨æ¨¡æ‹ŸæœåŠ¡æ¨¡å¼ï¼Œå»ºè®®å®‰è£…å®Œæ•´æ¨¡å‹ä»¥è·å¾—æ›´å‡†ç¡®çš„å›ç­”ï¼‰
        """.strip()
    
    def legal_consultation(self, question: str, context: str = None) -> Dict[str, Any]:
        """æ³•å¾‹å’¨è¯¢"""
        try:
            # æ„å»ºæç¤ºè¯
            if context:
                prompt = f"é—®é¢˜: {question}\n\nèƒŒæ™¯ä¿¡æ¯: {context}\n\nè¯·æä¾›è¯¦ç»†çš„æ³•å¾‹åˆ†æå’Œå»ºè®®ã€‚"
            else:
                prompt = question
            
            # ç”Ÿæˆå›å¤
            response = self.generate_response(prompt)
            
            # åˆ†æå›å¤è´¨é‡
            confidence = self._analyze_response_quality(response)
            
            return {
                "question": question,
                "answer": response,
                "confidence": confidence,
                "model": f"{self.model_name} (Windowså…¼å®¹ç‰ˆæœ¬)",
                "timestamp": datetime.now().isoformat(),
                "context": context
            }
            
        except Exception as e:
            logger.error(f"âŒ æ³•å¾‹å’¨è¯¢å¤±è´¥: {e}")
            return {
                "question": question,
                "answer": f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                "confidence": 0.0,
                "model": f"{self.model_name} (Windowså…¼å®¹ç‰ˆæœ¬)",
                "timestamp": datetime.now().isoformat(),
                "context": context
            }
    
    def legal_analysis(self, case_text: str) -> Dict[str, Any]:
        """æ³•å¾‹æ¡ˆä¾‹åˆ†æ"""
        try:
            prompt = f"è¯·åˆ†æä»¥ä¸‹æ³•å¾‹æ¡ˆä¾‹ï¼ŒåŒ…æ‹¬æ¡ˆä»¶æ€§è´¨ã€é€‚ç”¨æ³•å¾‹ã€å¯èƒ½çš„æ³•å¾‹åæœç­‰:\n\n{case_text}"
            
            response = self.generate_response(prompt)
            confidence = self._analyze_response_quality(response)
            
            return {
                "case_text": case_text,
                "analysis": response,
                "confidence": confidence,
                "model": f"{self.model_name} (Windowså…¼å®¹ç‰ˆæœ¬)",
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ æ³•å¾‹åˆ†æå¤±è´¥: {e}")
            return {
                "case_text": case_text,
                "analysis": f"æŠ±æ­‰ï¼Œåˆ†ææ¡ˆä¾‹æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                "confidence": 0.0,
                "model": f"{self.model_name} (Windowså…¼å®¹ç‰ˆæœ¬)",
                "timestamp": datetime.now().isoformat()
            }
    
    def legal_document_review(self, document_text: str) -> Dict[str, Any]:
        """æ³•å¾‹æ–‡æ¡£å®¡æŸ¥"""
        try:
            prompt = f"è¯·å®¡æŸ¥ä»¥ä¸‹æ³•å¾‹æ–‡æ¡£ï¼ŒæŒ‡å‡ºæ½œåœ¨çš„æ³•å¾‹é£é™©ã€åˆè§„é—®é¢˜å’Œæ”¹è¿›å»ºè®®:\n\n{document_text}"
            
            response = self.generate_response(prompt)
            confidence = self._analyze_response_quality(response)
            
            return {
                "document_text": document_text,
                "review": response,
                "confidence": confidence,
                "model": f"{self.model_name} (Windowså…¼å®¹ç‰ˆæœ¬)",
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ æ³•å¾‹æ–‡æ¡£å®¡æŸ¥å¤±è´¥: {e}")
            return {
                "document_text": document_text,
                "review": f"æŠ±æ­‰ï¼Œå®¡æŸ¥æ–‡æ¡£æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                "confidence": 0.0,
                "model": f"{self.model_name} (Windowså…¼å®¹ç‰ˆæœ¬)",
                "timestamp": datetime.now().isoformat()
            }
    
    def legal_research(self, research_topic: str) -> Dict[str, Any]:
        """æ³•å¾‹ç ”ç©¶"""
        try:
            prompt = f"è¯·å°±ä»¥ä¸‹æ³•å¾‹ç ”ç©¶ä¸»é¢˜æä¾›è¯¦ç»†çš„ç ”ç©¶æŠ¥å‘Šï¼ŒåŒ…æ‹¬ç›¸å…³æ³•å¾‹æ¡æ–‡ã€æ¡ˆä¾‹åˆ†æã€å­¦æœ¯è§‚ç‚¹ç­‰:\n\n{research_topic}"
            
            response = self.generate_response(prompt)
            confidence = self._analyze_response_quality(response)
            
            return {
                "research_topic": research_topic,
                "research_report": response,
                "confidence": confidence,
                "model": f"{self.model_name} (Windowså…¼å®¹ç‰ˆæœ¬)",
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ æ³•å¾‹ç ”ç©¶å¤±è´¥: {e}")
            return {
                "research_topic": research_topic,
                "research_report": f"æŠ±æ­‰ï¼Œè¿›è¡Œç ”ç©¶æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                "confidence": 0.0,
                "model": f"{self.model_name} (Windowså…¼å®¹ç‰ˆæœ¬)",
                "timestamp": datetime.now().isoformat()
            }
    
    def _analyze_response_quality(self, response: str) -> float:
        """åˆ†æå›å¤è´¨é‡"""
        try:
            # ç®€å•çš„è´¨é‡è¯„ä¼°æŒ‡æ ‡
            quality_score = 0.0
            
            # é•¿åº¦æ£€æŸ¥
            if len(response) > 50:
                quality_score += 0.2
            
            # æ³•å¾‹å…³é”®è¯æ£€æŸ¥
            legal_keywords = ["æ³•å¾‹", "æ³•è§„", "æ¡æ–‡", "è§„å®š", "æ¡æ¬¾", "æ¡ˆä¾‹", "åˆ¤å†³", "æ³•é™¢", "å¾‹å¸ˆ", "è¯‰è®¼"]
            keyword_count = sum(1 for keyword in legal_keywords if keyword in response)
            quality_score += min(keyword_count * 0.1, 0.4)
            
            # ç»“æ„æ£€æŸ¥
            if "ã€‚" in response and "ï¼Œ" in response:
                quality_score += 0.2
            
            # å®Œæ•´æ€§æ£€æŸ¥
            if len(response.split()) > 20:
                quality_score += 0.2
            
            return min(quality_score, 1.0)
            
        except Exception as e:
            logger.error(f"âŒ è´¨é‡åˆ†æå¤±è´¥: {e}")
            return 0.5
    
    def batch_consultation(self, questions: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡æ³•å¾‹å’¨è¯¢"""
        results = []
        
        for question in questions:
            try:
                result = self.legal_consultation(question)
                results.append(result)
            except Exception as e:
                logger.error(f"âŒ æ‰¹é‡å’¨è¯¢å¤±è´¥: {e}")
                results.append({
                    "question": question,
                    "answer": f"å¤„ç†å¤±è´¥: {str(e)}",
                    "confidence": 0.0,
                    "model": f"{self.model_name} (Windowså…¼å®¹ç‰ˆæœ¬)",
                    "timestamp": datetime.now().isoformat()
                })
        
        return results
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "model_name": self.model_name,
            "is_initialized": self.is_initialized,
            "generation_config": self.generation_config,
            "device": "cuda" if torch.cuda.is_available() else "cpu",
            "version": "Windowså…¼å®¹ç‰ˆæœ¬",
            "timestamp": datetime.now().isoformat()
        }

# å…¨å±€æœåŠ¡å®ä¾‹
lawllm_service_windows = LawLLMServiceWindows()

def get_lawllm_service() -> LawLLMServiceWindows:
    """è·å– LawLLM æœåŠ¡å®ä¾‹"""
    return lawllm_service_windows






