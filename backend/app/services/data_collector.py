"""
æ³•å¾‹æ•°æ®æ”¶é›†æœåŠ¡
ä»å„ç§æ¥æºæ”¶é›†æ³•å¾‹æ–‡æœ¬æ•°æ®ç”¨äºBERTè®­ç»ƒ
"""
import os
import json
import requests
from bs4 import BeautifulSoup
import time
import jieba
from typing import List, Dict, Any
import pandas as pd

class LegalDataCollector:
    """æ³•å¾‹æ•°æ®æ”¶é›†å™¨"""
    
    def __init__(self):
        self.data_dir = "./data/training"
        self.collected_data = []
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
    def collect_from_websites(self):
        """ä»ç²¤æ¸¯æ¾³å¤§æ¹¾åŒºæ³•å¾‹ç½‘ç«™æ”¶é›†æ•°æ®"""
        print("ğŸŒ å¼€å§‹ä»ç²¤æ¸¯æ¾³å¤§æ¹¾åŒºæ³•å¾‹ç½‘ç«™æ”¶é›†æ•°æ®...")
        
        # ç²¤æ¸¯æ¾³å¤§æ¹¾åŒºæ³•å¾‹ç½‘ç«™åˆ—è¡¨
        legal_sites = [
            {
                "name": "å¹¿ä¸œçœé«˜çº§äººæ°‘æ³•é™¢",
                "url": "http://www.gdcourts.gov.cn/",
                "selectors": {
                    "title": "h1, h2, h3, .title",
                    "content": "p, .content, .article-content"
                },
                "region": "å¹¿ä¸œ"
            },
            {
                "name": "æ·±åœ³å¸‚ä¸­çº§äººæ°‘æ³•é™¢",
                "url": "http://www.szcourt.gov.cn/",
                "selectors": {
                    "title": "h1, h2, h3, .title",
                    "content": "p, .content, .article-content"
                },
                "region": "æ·±åœ³"
            },
            {
                "name": "å¹¿å·å¸‚ä¸­çº§äººæ°‘æ³•é™¢",
                "url": "http://www.gzcourt.gov.cn/",
                "selectors": {
                    "title": "h1, h2, h3, .title",
                    "content": "p, .content, .article-content"
                },
                "region": "å¹¿å·"
            },
            {
                "name": "ç æµ·å¸‚ä¸­çº§äººæ°‘æ³•é™¢",
                "url": "http://www.zhcourt.gov.cn/",
                "selectors": {
                    "title": "h1, h2, h3, .title",
                    "content": "p, .content, .article-content"
                },
                "region": "ç æµ·"
            },
            {
                "name": "ä½›å±±å¸‚ä¸­çº§äººæ°‘æ³•é™¢",
                "url": "http://www.fscourt.gov.cn/",
                "selectors": {
                    "title": "h1, h2, h3, .title",
                    "content": "p, .content, .article-content"
                },
                "region": "ä½›å±±"
            },
            {
                "name": "ä¸œèå¸‚ä¸­çº§äººæ°‘æ³•é™¢",
                "url": "http://www.dgcourt.gov.cn/",
                "selectors": {
                    "title": "h1, h2, h3, .title",
                    "content": "p, .content, .article-content"
                },
                "region": "ä¸œè"
            },
            {
                "name": "ä¸­å±±å¸‚ä¸­çº§äººæ°‘æ³•é™¢",
                "url": "http://www.zscourt.gov.cn/",
                "selectors": {
                    "title": "h1, h2, h3, .title",
                    "content": "p, .content, .article-content"
                },
                "region": "ä¸­å±±"
            },
            {
                "name": "æƒ å·å¸‚ä¸­çº§äººæ°‘æ³•é™¢",
                "url": "http://www.hzcourt.gov.cn/",
                "selectors": {
                    "title": "h1, h2, h3, .title",
                    "content": "p, .content, .article-content"
                },
                "region": "æƒ å·"
            },
            {
                "name": "æ±Ÿé—¨å¸‚ä¸­çº§äººæ°‘æ³•é™¢",
                "url": "http://www.jmcourt.gov.cn/",
                "selectors": {
                    "title": "h1, h2, h3, .title",
                    "content": "p, .content, .article-content"
                },
                "region": "æ±Ÿé—¨"
            },
            {
                "name": "è‚‡åº†å¸‚ä¸­çº§äººæ°‘æ³•é™¢",
                "url": "http://www.zqcourt.gov.cn/",
                "selectors": {
                    "title": "h1, h2, h3, .title",
                    "content": "p, .content, .article-content"
                },
                "region": "è‚‡åº†"
            }
        ]
        
        for site in legal_sites:
            try:
                print(f"ğŸ“¡ æ­£åœ¨æ”¶é›†: {site['name']}")
                self._scrape_website(site)
                time.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«
            except Exception as e:
                print(f"âŒ æ”¶é›†å¤±è´¥ {site['name']}: {e}")
    
    def _scrape_website(self, site_config):
        """çˆ¬å–å•ä¸ªç½‘ç«™"""
        try:
            response = requests.get(site_config['url'], timeout=10, headers=self.headers)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–æ ‡é¢˜å’Œå†…å®¹
            titles = soup.select(site_config['selectors']['title'])
            contents = soup.select(site_config['selectors']['content'])
            
            for title, content in zip(titles, contents):
                if title.text.strip() and content.text.strip():
                    self.collected_data.append({
                        "text": f"{title.text.strip()} {content.text.strip()}",
                        "source": site_config['name'],
                        "region": site_config.get('region', 'æœªçŸ¥'),
                        "category": self._classify_text(title.text.strip()),
                        "timestamp": datetime.now().isoformat()
                    })
                    
        except Exception as e:
            print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
    
    def _classify_text(self, text: str) -> int:
        """ç®€å•çš„æ–‡æœ¬åˆ†ç±»"""
        # åŸºäºå…³é”®è¯çš„ç®€å•åˆ†ç±»
        keywords = {
            0: ["åˆåŒ", "åè®®", "å±¥è¡Œ", "è¿çº¦"],  # åˆåŒæ³•
            1: ["åŠ³åŠ¨", "å‘˜å·¥", "å·¥èµ„", "ç¤¾ä¿"],  # åŠ³åŠ¨æ³•
            2: ["ä¸“åˆ©", "å•†æ ‡", "ç‰ˆæƒ", "çŸ¥è¯†äº§æƒ"],  # çŸ¥è¯†äº§æƒ
            3: ["äº‹æ•…", "ä¾µæƒ", "èµ”å¿", "è´£ä»»"],  # ä¾µæƒæ³•
            4: ["å©šå§»", "ç¦»å©š", "è´¢äº§", "æŠšå…»"],  # å©šå§»æ³•
            5: ["å…¬å¸", "è‚¡ä¸œ", "è‘£äº‹", "ä¼ä¸š"],  # å…¬å¸æ³•
            6: ["çŠ¯ç½ª", "åˆ‘ç½š", "åˆ‘äº‹", "èµ·è¯‰"],  # åˆ‘æ³•
            7: ["è¡Œæ”¿", "æ”¿åºœ", "å¤„ç½š", "å¤è®®"],  # è¡Œæ”¿æ³•
            8: ["å›½é™…", "ä»²è£", "è´¸æ˜“", "æŠ•èµ„"],  # å›½é™…æ³•
            9: ["ç¯å¢ƒ", "æ±¡æŸ“", "ä¿æŠ¤", "ç”Ÿæ€"]   # ç¯å¢ƒæ³•
        }
        
        for category, words in keywords.items():
            if any(word in text for word in words):
                return category
        
        return 0  # é»˜è®¤åˆ†ç±»
    
    def create_synthetic_data(self):
        """åˆ›å»ºåˆæˆæ³•å¾‹æ•°æ®"""
        print("ğŸ“ åˆ›å»ºåˆæˆæ³•å¾‹æ•°æ®...")
        
        # æ³•å¾‹æ–‡æœ¬æ¨¡æ¿
        templates = {
            0: [  # åˆåŒæ³•
                "æ ¹æ®ã€ŠåˆåŒæ³•ã€‹ç¬¬{article}æ¡è§„å®šï¼Œ{subject}åº”å½“{action}",
                "åˆåŒåŒæ–¹åº”å½“æŒ‰ç…§çº¦å®š{action}ï¼Œå¦åˆ™æ‰¿æ‹…{consequence}",
                "åœ¨{scenario}æƒ…å†µä¸‹ï¼ŒåˆåŒ{result}"
            ],
            1: [  # åŠ³åŠ¨æ³•
                "ç”¨äººå•ä½åº”å½“ä¸ºåŠ³åŠ¨è€…{action}ï¼Œè¿åè€…æ‰¿æ‹…{consequence}",
                "æ ¹æ®ã€ŠåŠ³åŠ¨æ³•ã€‹è§„å®šï¼Œ{subject}äº«æœ‰{rights}",
                "åœ¨{scenario}æƒ…å†µä¸‹ï¼ŒåŠ³åŠ¨è€…å¯ä»¥{action}"
            ],
            2: [  # çŸ¥è¯†äº§æƒæ³•
                "ä¾µçŠ¯ä»–äºº{ip_type}æƒçš„ï¼Œåº”å½“æ‰¿æ‹…{consequence}",
                "æ ¹æ®ã€Š{law_name}ã€‹è§„å®šï¼Œ{subject}äº«æœ‰{rights}",
                "åœ¨{scenario}æƒ…å†µä¸‹ï¼Œ{ip_type}æƒå—åˆ°ä¿æŠ¤"
            ]
        }
        
        # å¡«å……è¯æ±‡
        fillers = {
            "article": ["å››ç™¾å…­åä¹æ¡", "äº”ç™¾é›¶ä¹æ¡", "äº”ç™¾å…­åä¸‰æ¡"],
            "subject": ["å½“äº‹äºº", "åˆåŒåŒæ–¹", "ç”¨äººå•ä½", "åŠ³åŠ¨è€…"],
            "action": ["å±¥è¡Œä¹‰åŠ¡", "æ”¯ä»˜è´¹ç”¨", "æä¾›ä¿éšœ", "éµå®ˆçº¦å®š"],
            "consequence": ["è¿çº¦è´£ä»»", "æ³•å¾‹è´£ä»»", "èµ”å¿è´£ä»»"],
            "scenario": ["åˆåŒå±¥è¡Œ", "åŠ³åŠ¨å…³ç³»", "çŸ¥è¯†äº§æƒä¿æŠ¤"],
            "result": ["æœ‰æ•ˆ", "æ— æ•ˆ", "å¯æ’¤é”€"],
            "rights": ["åˆæ³•æƒç›Š", "åŸºæœ¬æƒåˆ©", "æ³•å®šæƒåˆ©"],
            "ip_type": ["ä¸“åˆ©", "å•†æ ‡", "è‘—ä½œæƒ"],
            "law_name": ["ä¸“åˆ©æ³•", "å•†æ ‡æ³•", "è‘—ä½œæƒæ³•"]
        }
        
        synthetic_data = []
        for category, template_list in templates.items():
            for template in template_list:
                # ç”Ÿæˆå¤šä¸ªå˜ä½“
                for i in range(5):
                    text = template
                    for key, values in fillers.items():
                        if f"{{{key}}}" in text:
                            import random
                            text = text.replace(f"{{{key}}}", random.choice(values))
                    
                    synthetic_data.append({
                        "text": f"{text} - å˜ä½“{i+1}",
                        "category": category,
                        "source": "synthetic"
                    })
        
        self.collected_data.extend(synthetic_data)
        print(f"âœ… åˆ›å»ºäº† {len(synthetic_data)} æ¡åˆæˆæ•°æ®")
    
    def process_data(self):
        """å¤„ç†æ”¶é›†çš„æ•°æ®"""
        print("ğŸ”§ å¤„ç†æ”¶é›†çš„æ•°æ®...")
        
        processed_data = []
        for item in self.collected_data:
            # æ–‡æœ¬æ¸…æ´—
            text = self._clean_text(item['text'])
            if len(text) < 10:  # è¿‡æ»¤å¤ªçŸ­çš„æ–‡æœ¬
                continue
            
            # åˆ†è¯å¤„ç†
            words = jieba.lcut(text)
            
            processed_data.append({
                "text": text,
                "label": item.get('category', 0),
                "source": item.get('source', 'unknown'),
                "word_count": len(words)
            })
        
        self.collected_data = processed_data
        print(f"âœ… å¤„ç†å®Œæˆï¼Œå…± {len(processed_data)} æ¡æ•°æ®")
    
    def _clean_text(self, text: str) -> str:
        """æ¸…æ´—æ–‡æœ¬"""
        import re
        
        # ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[^\w\s\u4e00-\u9fff]', '', text)
        
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def save_data(self, filename: str = "legal_training_data.json"):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        os.makedirs(self.data_dir, exist_ok=True)
        filepath = os.path.join(self.data_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.collected_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self._generate_report()
    
    def _generate_report(self):
        """ç”Ÿæˆæ•°æ®ç»Ÿè®¡æŠ¥å‘Š"""
        if not self.collected_data:
            return
        
        df = pd.DataFrame(self.collected_data)
        
        print("\nğŸ“Š æ•°æ®ç»Ÿè®¡æŠ¥å‘Š:")
        print(f"æ€»æ•°æ®é‡: {len(df)}")
        print(f"å¹³å‡æ–‡æœ¬é•¿åº¦: {df['word_count'].mean():.1f} è¯")
        print("\næŒ‰ç±»åˆ«åˆ†å¸ƒ:")
        print(df['label'].value_counts().sort_index())
        print("\næŒ‰æ¥æºåˆ†å¸ƒ:")
        print(df['source'].value_counts())

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ”¶é›†æ³•å¾‹è®­ç»ƒæ•°æ®...")
    
    collector = LegalDataCollector()
    
    # 1. åˆ›å»ºåˆæˆæ•°æ®
    collector.create_synthetic_data()
    
    # 2. ä»ç½‘ç«™æ”¶é›†æ•°æ®ï¼ˆå¯é€‰ï¼‰
    # collector.collect_from_websites()
    
    # 3. å¤„ç†æ•°æ®
    collector.process_data()
    
    # 4. ä¿å­˜æ•°æ®
    collector.save_data()
    
    print("âœ… æ•°æ®æ”¶é›†å®Œæˆï¼")

if __name__ == "__main__":
    main()