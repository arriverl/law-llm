"""
BERTæ³•å¾‹æ¨¡å‹è®­ç»ƒæœåŠ¡
"""
import os
import json
import torch
import pandas as pd
from typing import List, Dict, Any
from transformers import (
    BertTokenizer, 
    BertForSequenceClassification,
    TrainingArguments, 
    Trainer,
    AutoTokenizer,
    AutoModelForSequenceClassification
)
from torch.utils.data import Dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import numpy as np

class LegalDataset(Dataset):
    """æ³•å¾‹æ–‡æœ¬æ•°æ®é›†"""
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

class LegalBERTTrainer:
    """æ³•å¾‹BERTæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.tokenizer = None
        self.model = None
        self.trainer = None
        
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨"""
        model_name = self.config.get('model_name', 'bert-base-chinese')
        
        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        
        # åŠ è½½æ¨¡å‹
        num_labels = self.config.get('num_labels', 10)
        self.model = BertForSequenceClassification.from_pretrained(
            model_name,
            num_labels=num_labels
        )
        
        print(f"âœ… æ¨¡å‹è®¾ç½®å®Œæˆ: {model_name}")
        print(f"ğŸ“Š åˆ†ç±»æ ‡ç­¾æ•°é‡: {num_labels}")
    
    def prepare_data(self, data_path: str):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        print("ğŸ“š å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        # è¯»å–æ•°æ®
        if data_path.endswith('.csv'):
            df = pd.read_csv(data_path)
        elif data_path.endswith('.json'):
            with open(data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            df = pd.DataFrame(data)
        else:
            raise ValueError("æ”¯æŒçš„æ•°æ®æ ¼å¼: CSV, JSON")
        
        # æ•°æ®é¢„å¤„ç†
        texts = df['text'].tolist()
        labels = df['label'].tolist()
        
        # åˆ†å‰²æ•°æ®é›†
        train_texts, val_texts, train_labels, val_labels = train_test_split(
            texts, labels, test_size=0.2, random_state=42
        )
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = LegalDataset(
            train_texts, train_labels, self.tokenizer, self.config.get('max_length', 512)
        )
        val_dataset = LegalDataset(
            val_texts, val_labels, self.tokenizer, self.config.get('max_length', 512)
        )
        
        print(f"ğŸ“ˆ è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
        print(f"ğŸ“ˆ éªŒè¯é›†å¤§å°: {len(val_dataset)}")
        
        return train_dataset, val_dataset
    
    def compute_metrics(self, eval_pred):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=1)
        
        precision, recall, f1, _ = precision_recall_fscore_support(
            labels, predictions, average='weighted'
        )
        accuracy = accuracy_score(labels, predictions)
        
        return {
            'accuracy': accuracy,
            'f1': f1,
            'precision': precision,
            'recall': recall
        }
    
    def train(self, train_dataset, val_dataset):
        """å¼€å§‹è®­ç»ƒ"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒBERTæ¨¡å‹...")
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=self.config.get('output_dir', './models/trained'),
            num_train_epochs=self.config.get('epochs', 3),
            per_device_train_batch_size=self.config.get('batch_size', 16),
            per_device_eval_batch_size=self.config.get('batch_size', 16),
            learning_rate=self.config.get('learning_rate', 2e-5),
            warmup_steps=self.config.get('warmup_steps', 500),
            logging_dir='./logs',
            logging_steps=100,
            eval_strategy="epoch",
            save_strategy="epoch",
            load_best_model_at_end=True,
            metric_for_best_model="f1",
            greater_is_better=True,
        )
        
        # åˆ›å»ºè®­ç»ƒå™¨
        self.trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            compute_metrics=self.compute_metrics,
        )
        
        # å¼€å§‹è®­ç»ƒ
        self.trainer.train()
        
        # ä¿å­˜æ¨¡å‹
        self.trainer.save_model()
        self.tokenizer.save_pretrained(self.config.get('output_dir', './models/trained'))
        
        print("âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜")
        
        # è¯„ä¼°æ¨¡å‹
        eval_results = self.trainer.evaluate()
        print("ğŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æœ:")
        for key, value in eval_results.items():
            print(f"  {key}: {value:.4f}")
    
    def predict(self, text: str):
        """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        if self.model is None or self.tokenizer is None:
            raise ValueError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè®­ç»ƒæˆ–åŠ è½½æ¨¡å‹")
        
        # ç¼–ç è¾“å…¥
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.config.get('max_length', 512),
            return_tensors='pt'
        )
        
        # é¢„æµ‹
        with torch.no_grad():
            outputs = self.model(**encoding)
            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
            predicted_class = torch.argmax(predictions, dim=-1).item()
            confidence = torch.max(predictions).item()
        
        return {
            'predicted_class': predicted_class,
            'confidence': confidence,
            'probabilities': predictions.numpy().tolist()
        }

def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹è®­ç»ƒæ•°æ®"""
    sample_data = [
        {"text": "åˆåŒåŒæ–¹åº”å½“æŒ‰ç…§çº¦å®šå±¥è¡Œä¹‰åŠ¡", "label": 0},  # åˆåŒæ³•
        {"text": "ç”¨äººå•ä½åº”å½“ä¸ºåŠ³åŠ¨è€…ç¼´çº³ç¤¾ä¼šä¿é™©", "label": 1},  # åŠ³åŠ¨æ³•
        {"text": "ä¾µçŠ¯ä»–äººçŸ¥è¯†äº§æƒåº”å½“æ‰¿æ‹…æ³•å¾‹è´£ä»»", "label": 2},  # çŸ¥è¯†äº§æƒæ³•
        {"text": "äº¤é€šäº‹æ•…è´£ä»»è®¤å®šéœ€è¦æ ¹æ®å…·ä½“æƒ…å†µ", "label": 3},  # ä¾µæƒæ³•
        {"text": "å©šå§»å…³ç³»å­˜ç»­æœŸé—´è´¢äº§å½’å±é—®é¢˜", "label": 4},  # å©šå§»æ³•
        {"text": "å…¬å¸è®¾ç«‹éœ€è¦æ»¡è¶³æ³•å®šæ¡ä»¶", "label": 5},  # å…¬å¸æ³•
        {"text": "åˆ‘äº‹æ¡ˆä»¶å®¡ç†ç¨‹åºåº”å½“åˆæ³•", "label": 6},  # åˆ‘æ³•
        {"text": "è¡Œæ”¿è¯‰è®¼ä¸­ä¸¾è¯è´£ä»»åˆ†é…", "label": 7},  # è¡Œæ”¿æ³•
        {"text": "å›½é™…å•†äº‹ä»²è£è£å†³æ‰§è¡Œ", "label": 8},  # å›½é™…æ³•
        {"text": "ç¯å¢ƒä¿æŠ¤æ³•å¾‹è´£ä»»è¿½ç©¶", "label": 9},  # ç¯å¢ƒæ³•
    ]
    
    # æ‰©å±•æ•°æ®
    extended_data = []
    for item in sample_data:
        # ä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºå¤šä¸ªå˜ä½“
        for i in range(10):
            extended_data.append({
                "text": f"{item['text']} - å˜ä½“{i+1}",
                "label": item['label']
            })
    
    return extended_data

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # è®­ç»ƒé…ç½®
    config = {
        'model_name': 'bert-base-chinese',
        'num_labels': 10,
        'max_length': 512,
        'epochs': 3,
        'batch_size': 16,
        'learning_rate': 2e-5,
        'warmup_steps': 500,
        'output_dir': './models/legal-bert-trained'
    }
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = LegalBERTTrainer(config)
    
    # è®¾ç½®æ¨¡å‹
    trainer.setup_model()
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    print("ğŸ“ åˆ›å»ºç¤ºä¾‹è®­ç»ƒæ•°æ®...")
    sample_data = create_sample_data()
    
    # ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶
    data_path = './data/training/legal_data.json'
    os.makedirs(os.path.dirname(data_path), exist_ok=True)
    
    with open(data_path, 'w', encoding='utf-8') as f:
        json.dump(sample_data, f, ensure_ascii=False, indent=2)
    
    # å‡†å¤‡æ•°æ®
    train_dataset, val_dataset = trainer.prepare_data(data_path)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(train_dataset, val_dataset)
    
    # æµ‹è¯•é¢„æµ‹
    print("\nğŸ§ª æµ‹è¯•æ¨¡å‹é¢„æµ‹:")
    test_texts = [
        "åˆåŒè¿çº¦åº”å½“æ‰¿æ‹…è¿çº¦è´£ä»»",
        "å‘˜å·¥åŠ ç­è´¹è®¡ç®—æ ‡å‡†",
        "å•†æ ‡ä¾µæƒæ¡ˆä»¶å¤„ç†"
    ]
    
    for text in test_texts:
        result = trainer.predict(text)
        print(f"æ–‡æœ¬: {text}")
        print(f"é¢„æµ‹ç±»åˆ«: {result['predicted_class']}")
        print(f"ç½®ä¿¡åº¦: {result['confidence']:.4f}")
        print("-" * 50)

if __name__ == "__main__":
    main()
